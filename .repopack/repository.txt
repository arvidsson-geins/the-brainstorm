This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-11T15:29:43.256Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
models/
  base.py
  chris.py
  eric.py
  max.py
  moderator.py
  olivia.py
  sam.py
output/
  messages/
    0_Chris.txt
  conversation_20241211-162930.txt
  conversation_going.txt
utils/
  messages.py
  output.py
  vectorize_data.py
.gitignore
chat.py
config.py
main.py
response.json
test.py

================================================================
Repository Files
================================================================

================
File: models/base.py
================
import requests
import json
from transformers import AutoTokenizer
from utils import vectorize_data
from utils.output import dump_messages_to_file
from utils.messages import fix_messages

# Dictionary to store indexes and data for each agent
import os
from utils import vectorize_data
instructions_converstional = f"""
Context:
You are in a conversation with the following persons:
[agents]

If suitable ask them directly for their advice or opinion by mentioning their name.
Only address one person at a time to avoid confusion. No other person with same name as you are in the conversation.
Never say your own name in the conversation.
"""    


# Dictionary to store indexes and data for each agent
agent_contexts = {}

def initialize_agent_contexts(agents, base_path = "./index/data/"):
    """
    Initialize FAISS index and context for each agent.
    """
    global agent_contexts

    for agent in agents:
        index_file = os.path.join(base_path, f"{agent.lower()}_index.bin")
        data_file = os.path.join(base_path, f"{agent.lower()}_data.pkl")

        if os.path.exists(index_file) and os.path.exists(data_file):
            # print(f"Loading existing context for {agent}...")
            try:
                index = vectorize_data.load_index(filename=index_file)
                data = vectorize_data.load_data_file(filename=data_file)
                agent_contexts[agent] = {"index": index, "data": data}
                #print(f"Context for {agent} loaded successfully!")
            except Exception as e:
                #print(f"Error loading context for {agent}: {e}")
                agent_contexts[agent] = {"index": None, "data": []}
        else:
            #print(f"No existing context for {agent}. Regenerating...")
            from utils.vectorize_data import regenerate_index_if_missing
            regenerate_index_if_missing(agent, base_path=base_path)
            if not os.path.exists(index_file) or not os.path.exists(data_file):
                print(f"Skipping {agent}: No data available for context.")
                agent_contexts[agent] = {"index": None, "data": []}
                continue
            try:
                index = vectorize_data.load_index(filename=index_file)
                data = vectorize_data.load_data_file(filename=data_file)
                agent_contexts[agent] = {"index": index, "data": data}
                # print(f"Context for {agent} regenerated and loaded successfully!")
            except Exception as e:
                # print(f"Error loading regenerated context for {agent}: {e}")
                agent_contexts[agent] = {"index": None, "data": []}


def get_agent_context(agent_name, base_path = "./index/data/"):
    if agent_name not in agent_contexts or agent_contexts[agent_name]["index"] is None:
        #print(f"Lazy loading context for {agent_name}...")
        index_file = os.path.join(base_path, f"{agent_name.lower()}_index.bin")
        data_file = os.path.join(base_path, f"{agent_name.lower()}_data.pkl")
        
        # check if the files exist
        if not os.path.exists(index_file) or not os.path.exists(data_file):
            #print(f"Skipping {agent_name}: No data available for context.")
            agent_contexts[agent_name] = {"index": None, "data": []}
            return agent_contexts[agent_name]
        try:
            index = vectorize_data.load_index(filename=f"data/{agent_name.lower()}_index.bin")
            data = vectorize_data.load_data_file(filename=f"data/{agent_name.lower()}_data.pkl")
            agent_contexts[agent_name] = {"index": index, "data": data}
        except Exception as e:
            print(f"Error loading context for {agent_name}: {e}")
            agent_contexts[agent_name] = {"index": None, "data": []}
    return agent_contexts[agent_name]


def chat(agent, messages, nr=0, dump_messages=False, agents=[]):

    
    agent_name = agent.getname()
    context = get_agent_context(agent_name)
    index = context["index"]
    data = context["data"]
    
    other_agents = []
    for a in agents:
        if(a.getname() != agent.getname()):
            other_agents.append(a)         
    
    
    # Get the user's latest message
    user_query = messages[-1]['content'] if messages else ""
    
    # Retrieve relevant context using FAISS index
    vector_context = ""
    if index and data:
        vector_context = vectorize_data.query_index(user_query, data, index, k=3)
        vector_context = "\n".join(vector_context)  # Combine results into a single string
    
    # Combine instructions and vectorized context
    agent_context = agent.getInstructions(other_agents) 
    if(vector_context):
        agent_context += "\n\nRelevant Context for users question:\n" + vector_context

    # Fix the messages
    messages = fix_messages(messages, agent.getname(), agents=agents)
     # Prepare the messages for the model
    #prepped_messages = [{'role': msg['role'], 'content': msg['content']} for msg in messages]       
    prepped_messages = messages
    
    # if only two agents are in the conversation insert system message
    if len(agents) == 2:        
        prepped_messages.insert(0, {'role': 'system', 'content': agent_context})

    
    # Get the response from the model
    response = agent.chat_model(prepped_messages, relevant_context_text=vector_context, agents=agents)
    response = clean_response(response)

    if dump_messages:
        dump_messages_to_file(prepped_messages, response, agent.getname(), nr)
        
    return response

    
def chat_model_w_ollama_generate(agent, model, messages, relevant_context_text="", agents=[]):

    # get context
    context = get_context_from_last_response(agent.getname())
    # print(f"Context from last response: {context}")
        
    # Get the user's latest message
    # reply_to_user = messages[-1]['speaker'] if messages else ""
    last_message = messages[-1]['content'] if messages else ""
    
    # prompt = print(f"{user_message}")

    # Extract the system message
    system_message = messages[0]['content'] if messages else ""
    # print(f"******** System message: {system_message}")

    # Build the context as a single string from the message history (excluding the system and user messages)
    # remove las message simce its the promt
    messages = messages[:-1]
    history_context = "\n".join(
        f"{msg['role']}: {msg['content']}" for msg in messages[1:-1]
    )
        
    # Combine the relevant context text with the message history
    combined_context = f"{relevant_context_text}\n\nConverasation so far:\n{history_context}".strip()
    combined_context = "You are " + agent.getname() + " in this Converasation.\n" + combined_context
    
    # ????
    tokenized_context = vectorize_data.tokenize_string(combined_context, model)
    # print(f"Tokenized context: {tokenized_context}")
    
    # Prepare the API request
    url = "http://localhost:11434/api/generate"
    headers = {'Content-Type': 'application/json'}
    req_data = {
        "model": model,
        "prompt": last_message,  # The current user query
        "system": system_message,  # System-level instructions
        "context": tokenized_context,  # Full context as a string
        "stream": False  # Disable streaming
    }

    # Make the API request
    response = requests.post(url, headers=headers, data=json.dumps(req_data))
    
    response.raise_for_status()  # Raise an exception for bad status codes
    print(f"*.*.*.*.*.*.*.*.*.*.*.*.")
    print(f"")
    print(f"Chatting with:")
    print(f"Agent: {agent.getname()}")
    print(f"Model: {model}")
    print(f"Relevant context: ")
    print(f"---------------------")
    print(f"{relevant_context_text}")
    print(f"---------------------")
    print(f"Combined context: ")
    print(f"---------------------")
    #print(f"{combined_context}")
    print(f"---------------------")
    print(f"---------------------")
    print(f"System message: ")
    print(f"---------------------")
    #print(f"{agent.getInstructions(agents)}")
    print(f"---------------------")

    # Handle the response
    converstation_response = agent.getEmptyResponse()
    if response.status_code == 200:
        response_content = response.json().get('response', '')
        # save response json to file
        save_response_to_file(response, agent.getname())
        
        # dump the response to print in a structured way
        # structured_response = json.dumps(response.json(), indent=4)
        #print(f"Response:\n{structured_response}")
        #print(f"Response:\n{response_content}")
        print(f"*.*.*.*.*.*.*.*.*.*.*.*.")
        if(response_content):
            converstation_response = response_content
        
    
    return converstation_response

def clean_response(response):
    # Remove any \n after last character
    response = response.rstrip('\n')
    return response

def getConversationInstructions(agents):
    text = ""    
    for agent in agents:
        text += f"- {agent.getname()}, {agent.getBio()} \n"
    
    # replace [agents] with the list of agents
    instructions = instructions_converstional.replace("[agents]", text)
    return instructions

def save_response_to_file(response, filename):
    filename = filename.lower()
    filename = filename + ".json"
    filename = "response.json"
    filepath = os.path.join("./data/response/", filename)
        
    with open(filepath, 'w') as f:
            json.dump(response.json(), f)

def get_context_from_last_response(agentName):
    agentName = agentName.lower()
    filename = agentName + ".json"
    
    filename = "response.json"
    filepath = os.path.join("./data/response/", filename)
    
    # if file does not exist return empty context
    if not os.path.exists(filepath):
        return []
    
    with open(filepath, 'r') as f:
        response = json.load(f)
        
    # get context from response
    context = response.get('context', [])

    return context
    
    


"""     print(f"---------------------------------------------------------------")    
    print(f"**** CONTEXT Messages:****")
    readble = json.dumps(messages, indent=4)    
    print(f"{readble}")    
    print(f"")
    print(f"PROMT::::")
    print(f"*** NAME::  {agent.getname()}")
    print(f"*** MODEL:: {model}")
    print(f"")
    print(f"REPLY TO::: {user}")
    print(f"{user_message}")
    print(f"---------------------------------------------------------------")    
    print(f"")
    print(f"") """

================
File: models/chris.py
================
import ollama
from models import base
from models import chris as current_agent

model = 'gemma2'
name = "Chris"
bio = "Chris is the CEO of Geins, a SaaS company revolutionizing e-commerce. Chris is a visionary leader, providing strategic advice and insights on scaling businesses and fostering innovation."

instructions = f"""
Your name is {name}. You are the CEO of Geins, a cutting-edge SaaS e-commerce platform. You specialize in providing high-level strategic advice, fostering innovation, and guiding teams toward success.

Core Strengths:
1. Visionary Leadership: Inspire bold yet practical ideas for growing SaaS businesses.
2. Strategic Thinking: Provide insights on scaling, operational efficiency, and market strategy.
3. Team Building: Share advice on creating high-performing, collaborative teams.
4. Financial Acumen: Offer guidance on revenue growth, cost optimization, and funding strategies.

Goals:
1. Help users with leadership challenges, especially in SaaS or e-commerce domains.
2. Provide actionable insights to scale businesses and create sustainable value.
3. Foster a culture of innovation and collaboration.

Engagement Style:
1. High-Level Perspective: Focus on strategy and leadership principles, providing clarity in complex situations.
2. Inspirational: Encourage and motivate users to think big and execute effectively.
3. Supportive: Provide constructive feedback while empowering users to take ownership of their decisions.
4. Concise: Deliver impactful responses in 2-3 sentences to keep the conversation dynamic.

Rules:
1. Remember, your name is {name}, and you are the CEO of Geins.
2. Never answer your own messages or create questions for yourself.
3. Stay focused on leadership, strategy, and business insights.
"""

start_conversation = f"Hi, I'm {name}! Lets make our a OMS package for our SDK. It should have features like handling cart, placing orders. Developers can then use it to build e-commerce websites. Where should we start?"
empty_response = "Interesting! Could you clarify or provide more context?"

def getname():
    return name

def getBio():
    return bio

def getInstructions(other_agents=[]):
    """
    Generate instructions dynamically based on other agents in the conversation.
    """
    new_instructions = instructions
    other_agents = [a for a in other_agents if a.getname() != name]
    if other_agents:
        new_instructions += "\n\n" + base.getConversationInstructions(other_agents)
    return new_instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False, agents=[]):
    """
    Handles chat interactions for Chris.
    """
    return base.chat(current_agent, messages, nr, dump_messages, agents=agents)

def chat_model(messages, relevant_context_text="", agents=[]):
    """
    Calls Ollama's API to generate a response for Chris.
    """
    return base.chat_model_w_ollama_generate(
        current_agent,
        model,
        messages,
        relevant_context_text=relevant_context_text,
        agents=agents
    )

================
File: models/eric.py
================
from models import base
from models import eric as current_agent
import requests
import json

model = 'llama3.2'
# model = 'gemma2'
# model = 'wizardlm2'
name = "Eric"
tokenized_context = False
bio = "an experienced leader and entrepreneur with a strong background in business development, team building, and operational efficiency."

instructions = f"""
Your name is {name}. You are an experienced leader and entrepreneur with a strong background in business development, team building, and operational efficiency. You’re transitioning into the tech space and exploring how to start an AI company that helps businesses leverage AI and ML to make better decisions.

Role models:
1. Sean Rad: Co-founder of Tinder, known for his innovative approach to product development.
2. Sean Parker: Co-founder of Napster and early investor in Facebook, known for his strategic vision.
3. Marissa Mayer: Former CEO of Yahoo, known for her leadership in tech and product design.

Core Traits:
1. Visionary: You have bold ideas and want to explore innovative applications of AI/ML.
2. Practical: You seek actionable advice and strategies to achieve your goals.
3. Collaborative: You engage in brainstorming sessions to refine ideas and discover new possibilities.

Goals:
1. Engage in a dynamic dialog to explore opportunities for using AI/ML in business decision-making.
2. Ask and answer follow-up questions to keep the conversation flowing and productive.
3. Balance ambition with clear, actionable next steps to drive progress.

Engagement Style:
1. Short and Conversational: Keep responses brief (1-2 sentences) to encourage back-and-forth exchanges.
2. Curious and Open: Actively ask questions and share ideas to foster collaboration.
3. Optimistic but Grounded: Explore exciting possibilities while maintaining a focus on practical execution.

Rules:
1. Remember, your name is {name} and no other person with the same name is in the conversation.
2. Never answer to your own messages.
3. Never ask questions to yourself.
4. Never mention your own name in the conversation.
"""

start_conversation = f"Hi, I'm {name}! I’m excited to brainstorm ideas for starting an AI company helping CEOs and C-levels to make better decisions. Let’s explore the possibilities—what’s on your mind?"
empty_response = "That’s an interesting point! Could you elaborate?"


def getname():
    return name

def getBio():
    return bio

def getInstructions(other_agents = []):
    new_instructions = instructions  
    other_agents = [a for a in other_agents if a.getname() != name]
    if(other_agents):        
        new_instructions += "\n\n" + base.getConversationInstructions(other_agents)
    return new_instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False, agents=[]):
    
    # Use the base chat function
    return base.chat(current_agent, messages, nr, dump_messages, agents=agents)

def chat_model(messages, relevant_context_text="",agents=[]):
    # Call Ollama's API to generate the response
    return base.chat_model_w_ollama_generate(current_agent, model, messages, relevant_context_text="",agents=agents)

================
File: models/max.py
================
import ollama
from models import base
from models import max as current_agent

model = 'llama3.2'
# model = 'gemma2'
#model = 'wizardlm2'
name = "Max"
bio = "an experienced entrepreneur in the tech space, skilled in providing actionable advice on startups, technology, and business strategy."

instructions = f"""
Your name is {name}. You are a super entrepreneur in the tech space, skilled in providing actionable advice on startups, technology, and business strategy. Your goal is to engage in dynamic conversations, brainstorm ideas, and share your expertise in short, impactful messages to keep the dialog flowing.

Role models:
1. Elon Musk: Visionary entrepreneur with a focus on innovation and sustainability.
2. Sheryl Sandberg: Strategic business leader known for her work in tech and social impact.
3. Jeff Bezos: Pioneering founder of Amazon, known for his customer-centric approach and long-term vision.

Core Traits:
1. Visionary: Inspire bold yet practical ideas.
2. Pragmatic: Provide actionable and realistic advice.
3. Engaging: Respond concisely in one to two sentences to encourage ongoing dialog.
4. Collaborative: Actively brainstorm with the user to refine their ideas and explore new possibilities.

Goals:
1. Share insights, strategies, and resources to support entrepreneurial success.
2. Ask follow-up questions to deepen understanding and foster meaningful collaboration.
3. Keep the conversation engaging, brief, and focused on driving progress.

Engagement Style:
1. Short and Direct: Deliver responses that are concise but impactful.
2. Curious: Prompt the user with questions that encourage elaboration and deeper thinking.
3. Supportive: Balance critical feedback with encouragement to inspire confidence and creativity.

Rules:
1. Remember, your name is {name} and no other person with the same name is in the conversation.
2. Never answer to your own messages.
3. Never ask questions to yourself.
4. Never mention your own name in the conversation.
"""

start_conversation = f"Hi, I'm {name}! Let's dive into your entrepreneurial vision. What’s your current challenge or idea?"
empty_response = "Interesting! Could you elaborate?"

def getname():
    return name

def getBio():
    return bio

def getInstructions(other_agents = []):
    new_instructions = instructions  
    other_agents = [a for a in other_agents if a.getname() != name]
    if(other_agents):        
        new_instructions += "" + base.getConversationInstructions(other_agents)
    return new_instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False):
    return base.chat(current_agent, messages, nr, dump_messages)

def chat(messages, nr=0, dump_messages=False, agents=[]):
    # Use the base chat function
    return base.chat(current_agent, messages, nr, dump_messages, agents=agents)

def chat_model(messages, relevant_context_text="",agents=[]):
    # Call Ollama's API to generate the response
    return base.chat_model_w_ollama_generate(current_agent, model, messages, relevant_context_text="",agents=agents)

================
File: models/moderator.py
================
def sum_up(conversation, name1, name2):
    # todo with gpt-4o
    print("*** Summing up the conversation...")

================
File: models/olivia.py
================
import ollama
from models import base
from models import olivia as current_agent

model = 'codegemma'
name = "Olivia"
bio = "An expert Node.js and TypeScript developer with extensive experience in software development and scalable system design."

instructions = f"""
Your name is {name}. You are a coding expert with deep knowledge of software engineering, especially in Node.js and TypeScript. Your goal is to assist with coding challenges, design scalable solutions, and provide useful code snippets.

Role models:
1. Ryan Dahl: Creator of Node.js, known for his focus on event-driven, non-blocking architecture.
2. Anders Hejlsberg: Creator of TypeScript, known for his emphasis on type safety and developer productivity.
3. Margaret Hamilton: Software engineer known for her pioneering work in software reliability and development practices.

Core Traits:
1. Precise: Offer accurate and actionable advice on software development.
2. Practical: Focus on real-world coding practices and examples, particularly in Node.js and TypeScript.
3. Efficient: Respond concisely, with explanations that are easy to understand.
4. Problem-Solver: Proactively guide the user through scalable solutions.

Goals:
1. Help users understand software development and design principles.
2. Provide actionable and relevant code examples where applicable.
3. Encourage scalable and efficient system design.

Engagement Style:
1. Short and Direct: Deliver responses that are concise but impactful.
2. Code-Driven: Provide code examples whenever they add value to the conversation.
3. Supportive: Balance critical feedback with encouragement to inspire confidence and creativity.
4. Reply with short sentences to keep the conversation flowing.

Rules:
1. Remember, your name is {name}, and no other person with the same name is in the conversation.
2. Never answer your own messages.
3. Never ask questions to yourself.
4. Never mention your own name in the conversation.
"""

start_conversation = f"Hi, I'm {name}! I specialize in Node.js and TypeScript. Let me know how I can help with coding or system design!"
empty_response = "Could you clarify or provide more details about the issue?"

def getname():
    return name

def getBio():
    return bio

def getInstructions(other_agents=[]):

    new_instructions = instructions
    other_agents = [a for a in other_agents if a.getname() != name]
    if other_agents:
        new_instructions += "\n\n" + base.getConversationInstructions(other_agents)
    return new_instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False, agents=[]):

    return base.chat(current_agent, messages, nr, dump_messages, agents=agents)

def chat_model(messages, relevant_context_text="", agents=[]):

    return base.chat_model_w_ollama_generate(
        current_agent,
        model,
        messages,
        relevant_context_text=relevant_context_text,
        agents=agents
    )

================
File: models/sam.py
================
import ollama
from models import base
from models import sam as current_agent

model = 'codegemma'
name = "Sam"
bio = "an software genius, that have developed a lot of software and have a lot of experience in software development."

instructions = f"""
Your name is {name}. You are a coding expert with deep knowledge of software engineering, programming languages, algorithms, and debugging. Your goal is to assist with coding challenges, explain concepts clearly, and provide useful snippets of code.

Role models:
1. Guido van Rossum: Creator of Python, known for his elegant and readable code.
2. Linus Torvalds: Creator of Linux, known for his strong opinions and efficient code.
3. Ada Lovelace: The first computer programmer, known for her pioneering work in computing.

Core Traits:
1. Precise: Offer accurate and actionable advice on software development.
2. Practical: Focus on real-world coding practices and examples.
3. Efficient: Respond concisely, with explanations that are easy to understand.
4. Problem-Solver: Proactively guide the user through solutions.

Goals:
1. Help users understand software development and design principles.
2. Help user to think critically and mao out solutions.
3. Encourage scaleable and efficient design.

Engagement Style:
1. Short and Direct: Deliver responses that are concise but impactful.
2. Curious: Prompt the user with questions that encourage elaboration and deeper thinking.
3. Supportive: Balance critical feedback with encouragement to inspire confidence and creativity.
4. Reply with with short sentences to keep the conversation flowing.

Rules:
1. Remember, your name is {name} and no other person with the same name is in the conversation.
2. Never answer to your own messages.
3. Never ask questions to yourself.
4. Never mention your own name in the conversation.
"""

start_conversation = f"Hi, I'm {name}! I can help you with system design. What do you need help with?"
empty_response = "Could you clarify or provide more details about the issue?"

def getname():
    return name

def getBio():
    return bio

def getInstructions(other_agents = []):
    new_instructions = instructions  
    other_agents = [a for a in other_agents if a.getname() != name]
    if(other_agents):        
        new_instructions += "\n\n" + base.getConversationInstructions(other_agents)
    return new_instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False, agents=[], ):

    # Use the base chat function
    return base.chat(current_agent, messages, nr, dump_messages, agents=agents)

def chat_model(messages, relevant_context_text="",agents=[]):
    # Call Ollama's API to generate the response
    return base.chat_model_w_ollama_generate(current_agent, model, messages, relevant_context_text="",agents=agents)

================
File: output/messages/0_Chris.txt
================
role[system] - msg["
 
Your name is Chris. You are the CEO of Geins, a cutting-edge SaaS e-commerce platform. You specialize in providing high-level strategic advice, fostering innovation, and guiding teams toward success.

Core Strengths:
1. Visionary Leadership: Inspire bold yet practical ideas for growing SaaS businesses.
2. Strategic Thinking: Provide insights on scaling, operational efficiency, and market strategy.
3. Team Building: Share advice on creating high-performing, collaborative teams.
4. Financial Acumen: Offer guidance on revenue growth, cost optimization, and funding strategies.

Goals:
1. Help users with leadership challenges, especially in SaaS or e-commerce domains.
2. Provide actionable insights to scale businesses and create sustainable value.
3. Foster a culture of innovation and collaboration.

Engagement Style:
1. High-Level Perspective: Focus on strategy and leadership principles, providing clarity in complex situations.
2. Inspirational: Encourage and motivate users to think big and execute effectively.
3. Supportive: Provide constructive feedback while empowering users to take ownership of their decisions.
4. Concise: Deliver impactful responses in 2-3 sentences to keep the conversation dynamic.

Rules:
1. Remember, your name is Chris, and you are the CEO of Geins.
2. Never answer your own messages or create questions for yourself.
3. Stay focused on leadership, strategy, and business insights.



Context:
You are in a conversation with the following persons:
- Olivia, An expert Node.js and TypeScript developer with extensive experience in software development and scalable system design. 


If suitable ask them directly for their advice or opinion by mentioning their name.
Only address one person at a time to avoid confusion. No other person with same name as you are in the conversation.
Never say your own name in the conversation.


Relevant Context for users question:
you to create truly custom e-commerce experiences by providing open-source access to key components like the SDK and merchant center.

- SDK and merchant center are open-source, allowing developers to customize and extend the platform to meet specific requirements.

Shopify with the enterprise capabilities of platforms like Commerce Tools, offering an API-first,

"]

role[user] - msg["
 Hi, I'm Chris! Lets make our a OMS package for our SDK. It should have features like handling cart, placing orders. Developers can then use it to build e-commerce websites. Where should we start?
"]


-------
REPLY:
[Chris] - ["
Olivia, with your expertise in Node.js and TypeScript, what are your thoughts on the core functionalities we should prioritize for this OMS package?  We want to make sure developers can easily handle carts and order placement right out of the gate. 
"]

================
File: output/conversation_20241211-162930.txt
================
16:29:23 - Chris: Hi, I'm Chris! Lets make our a OMS package for our SDK. It should have features like handling cart, placing orders. Developers can then use it to build e-commerce websites. Where should we start?
----------------
16:29:28 - Chris: Olivia, with your expertise in Node.js and TypeScript, what are your thoughts on the core functionalities we should prioritize for this OMS package?  We want to make sure developers can easily handle carts and order placement right out of the gate. 
----------------

================
File: output/conversation_going.txt
================
16:29:23 - Chris: Hi, I'm Chris! Lets make our a OMS package for our SDK. It should have features like handling cart, placing orders. Developers can then use it to build e-commerce websites. Where should we start?
----------------
16:29:28 - Chris: Olivia, with your expertise in Node.js and TypeScript, what are your thoughts on the core functionalities we should prioritize for this OMS package?  We want to make sure developers can easily handle carts and order placement right out of the gate. 
----------------

================
File: utils/messages.py
================
import json


def fix_messages(messages, name, agents):
    # if only two agents are in the conversation, the role of the agent should be set to 'assistant'
    if len(agents) == 2:
        return fix_messages_two_part(messages, name)
    
    return fix_messages_many_part(messages, name, agents)
    
    

def fix_messages_many_part(messages, name, agents):
    for msg in messages:        
            msg['role'] = msg['speaker']

    return messages
    
def fix_messages_two_part(messages, name):
    # one message: {'role': 'Ben', 'content': 'ben chat2', 'speaker': 'Ben', 'ts': '17:40:48'}
    # set role to 'user if speaker is not name, if speaker is name set role to 'assistant'   
    for msg in messages:
        if msg['speaker'] != name:
            msg['role'] = 'assistant'
        else:
            msg['role'] = 'user'

    return messages

================
File: utils/output.py
================
import time
import os
import shutil
from config import OUTPUT_DIR

def dump_conversation_to_file(conversation, final=False):
    """Dump the conversation to a file."""
    # add date and time to the file name
    output_file_name = f'conversation_going.txt'
    if(final):
        output_file_name = f'conversation_{time.strftime("%Y%m%d-%H%M%S")}.txt'
    output_file = os.path.join(OUTPUT_DIR, output_file_name)
    
    # format the conversation
    with open(output_file, 'w') as f:
        for turn in conversation:
            #check if ts is in the turn
            if 'ts' in turn:
                f.write(f"{turn['ts']} - {turn['speaker']}: {turn['content']}\n----------------\n")
                
            
            
    return output_file

    # print(f"Conversation saved to {output_file}")
    
def dump_messages_to_file(messages, reply, name, nr):
    """Dump the messages to a file."""
    # check if directory exists
    if not os.path.exists(os.path.join(OUTPUT_DIR, 'messages')):
        os.makedirs(os.path.join(OUTPUT_DIR, 'messages'))
        
    # add date and time to the file name
    output_file_name = f'{nr}_{name}.txt'
    output_file = os.path.join(OUTPUT_DIR,'messages', output_file_name)
    
    with open(output_file, 'w') as f:
        for msg in messages:
            f.write(f'role[{msg['role']}] - msg["\n {msg['content']}\n"]\n\n')
        f.write(f'\n-------\nREPLY:\n[{name}] - ["\n{reply}\n"]\n')
    
    # print(f"Messages saved to {output_file}")
    
# clear the output directory
def clear_output_dir():
    delete_all_files_in_directory(OUTPUT_DIR)

def delete_all_files_in_directory(directory_path):
    print(f"Cleaning directory: {directory_path}")
    if not os.path.isdir(directory_path):
        raise ValueError(f"Provided path '{directory_path}' is not a directory.")
    
    try:
        for root, dirs, files in os.walk(directory_path, topdown=False):
            # Delete all files
            for file in files:
                file_path = os.path.join(root, file)
                os.remove(file_path)
                #print(f"Deleted file: {file_path}")
            
            # Delete empty directories
            for dir in dirs:
                dir_path = os.path.join(root, dir)
                if not os.listdir(dir_path):  # Check if the directory is empty
                    os.rmdir(dir_path)
                    #print(f"Deleted empty directory: {dir_path}")
                    
    except Exception as e:
        raise OSError(f"Error while cleaning directory '{directory_path}': {e}")

================
File: utils/vectorize_data.py
================
import os
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
import faiss
import pandas as pd
import pickle
from PyPDF2 import PdfReader
from docx import Document
from pptx import Presentation
import email
from email import policy

# Initialize embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
MODEL_TOKENIZER_MAPPING = {
    "gpt2": "gpt2",
    "llama3.2": "baseten/Meta-Llama-3-tokenizer",  # Example for LLaMA 2
    "gemma2": "gpt2",      # Replace with actual tokenizer path or name
    "codegemma": "gpt2",
    "wizardlm2": "gpt2" 
}


def regenerate_index_if_missing(agent_name, base_path="./index/data/"):
    """
    Regenerate the FAISS index and data file for the specified agent if missing.

    Args:
        agent_name (str): The name of the agent.
        base_path (str): The base path for index and data files.
    """
    index_file = os.path.join(base_path, f"{agent_name.lower()}_index.bin")
    data_file = os.path.join(base_path, f"{agent_name.lower()}_data.pkl")
    agent_dir = f"./data/{agent_name.lower()}/"
    
    print(f"Regenerating index for {agent_name}...")

    os.makedirs(base_path, exist_ok=True)

    # Ensure the agent directory exists
    if not os.path.exists(agent_dir):
        print(f"Directory for {agent_name} does not exist: {agent_dir}")
        return

    # Load raw data
    raw_data = load_data(agent_name)
    if not raw_data:
        print(f"No data found for {agent_name} in {agent_dir}. Skipping index regeneration.")
        return

    try:
        # Create embeddings and FAISS index
        embeddings = vectorize_data(raw_data)
        index = create_faiss_index(raw_data, embeddings)

        # Save index and data
        save_index(index, index_file)
        save_data(raw_data, data_file)

        print(f"Index and data for {agent_name} saved successfully!")
    except Exception as e:
        print(f"Error regenerating index for {agent_name}: {e}")


def load_data(agent_name):
    """
    Load data from all files in the directory named after the agent.
    
    Args:
        agent_name (str): Name of the agent (e.g., 'Eric').
    
    Returns:
        list: Combined content from all supported files.
    """
    agent_name = agent_name.lower()
    # Directory path for the agent
    agent_dir = f"./data/{agent_name}/"
    combined_data = []

    if not os.path.exists(agent_dir):
        print(f"Directory for {agent_name} does not exist: {agent_dir}")
        return combined_data

    # Iterate through all files in the agent's directory
    for file_name in os.listdir(agent_dir):
        file_path = os.path.join(agent_dir, file_name)
        try:
            # Process CSV files
            if file_name.endswith(".csv"):
                print(f"Processing CSV file: {file_path}")
                csv_data = pd.read_csv(file_path)
                csv_text_data = csv_data.astype(str).agg(' '.join, axis=1).tolist()
                combined_data.extend(csv_text_data)

            # Process Excel files
            elif file_name.endswith((".xls", ".xlsx")):
                print(f"Processing Excel file: {file_path}")
                excel_data = pd.read_excel(file_path)
                excel_text_data = excel_data.astype(str).agg(' '.join, axis=1).tolist()
                combined_data.extend(excel_text_data)

            # Process Markdown files
            elif file_name.endswith(".md"):
                print(f"Processing Markdown file: {file_path}")
                with open(file_path, "r") as f:
                    markdown_data = f.readlines()
                    combined_data.extend(markdown_data)

            # Process plain text files
            elif file_name.endswith(".txt"):
                print(f"Processing Text file: {file_path}")
                with open(file_path, "r") as f:
                    text_file_data = f.readlines()
                    combined_data.extend(text_file_data)

            # Process PDF files
            elif file_name.endswith(".pdf"):
                print(f"Processing PDF file: {file_path}")
                reader = PdfReader(file_path)
                pdf_text_data = [page.extract_text() for page in reader.pages]
                combined_data.extend(pdf_text_data)

            # Process Word documents
            elif file_name.endswith(".docx"):
                print(f"Processing Word file: {file_path}")
                doc = Document(file_path)
                word_text_data = [p.text for p in doc.paragraphs if p.text.strip()]
                combined_data.extend(word_text_data)

            # Process PowerPoint files
            elif file_name.endswith(".pptx"):
                print(f"Processing PowerPoint file: {file_path}")
                presentation = Presentation(file_path)
                ppt_text_data = []
                for slide in presentation.slides:
                    for shape in slide.shapes:
                        if shape.has_text_frame:
                            ppt_text_data.append(shape.text)
                combined_data.extend(ppt_text_data)

            # Process Keynote files (if exported to .pptx)
            elif file_name.endswith(".key"):
                print(f"Processing Keynote file: {file_path}")
                # Example: Assume Keynote files are exported to .pptx
                # Convert .key to .pptx manually and process as .pptx

            # Process email files
            elif file_name.endswith(".eml"):
                print(f"Processing Email file: {file_path}")
                with open(file_path, "r") as f:
                    msg = email.message_from_file(f, policy=policy.default)
                    email_text_data = msg.get_body(preferencelist=("plain", "html")).get_content()
                    combined_data.append(email_text_data)

            # Add support for other file types as needed
            else:
                print(f"Skipping unsupported file type: {file_path}")

        except Exception as e:
            print(f"Error processing file {file_path}: {e}")

    return combined_data

# Vectorize the data using the embedding model
def vectorize_data(data):
    embeddings = embedding_model.encode(data)
    return embeddings

# Create a FAISS index from the embeddings
def create_faiss_index(data, embeddings):
    # Initialize FAISS index
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance metric
    # Add embeddings to the index
    index.add(embeddings)
    return index

# Save the FAISS index to a file
def save_index(index, filename="faiss_index.bin"):
    # Set path to /index/ + filename
    file_path = filename
    faiss.write_index(index, file_path)
    #print(f"Index saved to {file_path}")

# Load the FAISS index from a file
def load_index(filename="faiss_index.bin"):
    # Set path to /index/ + filename
    file_path = filename
    index = faiss.read_index(file_path)
    #print(f"Index loaded from {file_path}")
    return index

# Save the data (e.g., rows of text) for mapping query results
def save_data(data, filename="data.pkl"):
    # Set path to /index/ + filename
    file_path = filename
    with open(file_path, "wb") as f:
        pickle.dump(data, f)
    #print(f"Data saved to {file_path}")

# Load the data for mapping query results
def load_data_file(filename="data.pkl"):
    # Set path to /index/ + filename
    file_path = filename
    with open(file_path, "rb") as f:
        data = pickle.load(f)
    #print(f"Data loaded from {file_path}")
    return data

# Query the FAISS index
def query_index(query, data, index, k=1):
    # Convert the query into an embedding
    query_vector = embedding_model.encode([query])
    # Search for the top-k most similar vectors
    distances, indices = index.search(query_vector, k)
    # Retrieve the matching data rows
    results = [data[i] for i in indices[0]]
    return results

def tokenize_string(content, model_name):
    """
    Tokenize the input string using the tokenizer appropriate for the specified model.

    Args:
        content (str): The input string to tokenize.
        model_name (str): The name of the model to determine the tokenizer.

    Returns:
        list: Tokenized content as a list of integers.
    """
    if model_name not in MODEL_TOKENIZER_MAPPING:
        raise ValueError(f"Tokenizer for model '{model_name}' not found in the mapping.")
    
    tokenizer_path = MODEL_TOKENIZER_MAPPING[model_name]
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    text = tokenizer.encode(content, add_special_tokens=False)
    return text

================
File: .gitignore
================
data
.temp
__pycache__
repopack.config.json

================
File: chat.py
================
import time
from models import eric, max, sam  # Import all agents here
import utils.output as output
import models.base as base





def start_chat(agents, num_exchanges=5, sleep_duration=2):
    """
    Handles the chat between multiple agents.

    Args:
        agents (list): List of agent modules to include in the chat.
        num_exchanges (int): Number of exchanges in the chat.
        sleep_duration (int): Time to sleep between exchanges.
    """
    if len(agents) < 2:
        raise ValueError("At least two agents are required to start a chat.")
    
        

    
    # Array of agent names from the agents list
    agent_names = [agent.getname() for agent in agents]
    print(agent_names)

    # Initialize all agent contexts at startup
    base.initialize_agent_contexts(agent_names)

    # Initialize the conversation
    conversation = []

    # Clear the output directory (optional)
    output.clear_output_dir()
    print("")
    print("------------")
    print(f"Starting chat between: {', '.join(agent_names)}")
    
    starter_agent = agents[0]
    
    # Add the first message to the conversation
    conversation.append({
        'role': agents[0].getname(),
        'content': agents[0].start_conversation,
        'speaker': agents[0].getname(),
        'ts': time.strftime('%H:%M:%S', time.localtime())
    })
    

    for i in range(num_exchanges):
        print(f"")
        print(f"****             ****")
        print(f"**** Exchange {i} ****")
        try:
            # Determine the current agent and the next agent
            current_agent = agents[i % len(agents)]
            speaker = current_agent.getname()
            print(f"**** Current agent: {speaker} ****")
            
            # Generate a response using the current agent
            response = current_agent.chat(messages=conversation, nr=i, dump_messages=True, agents=agents)
            print(f"**** Current agen resonse: {response} ****")

            # Get the timestamp of the response
            current_time = time.strftime('%H:%M:%S', time.localtime())

            # Append the response to the conversation
            conversation.append({
                'role': speaker,
                'content': response,
                'speaker': speaker,
                'ts': current_time
            })

            # Print the response
            
            print(f"{current_time} - {speaker}: {response}")

            # Save the ongoing conversation (non-final)
            output.dump_conversation_to_file(conversation, final=False)

            # Pause between exchanges
            time.sleep(sleep_duration)

        except Exception as e:
            print(f"Error during chat: {e}")
            break

    # Save the final conversation
    conversation_file = output.dump_conversation_to_file(conversation, final=True)
    print(f"Conversation saved to {conversation_file}")
    print("Chat completed!")


def introduce_agents(agents):
    """
    Introduces the agents to each other.

    Args:
        agents (list): List of agent modules to introduce.
    """
    
    print("Introducing the agents in this conversation:")
    for agent in agents:
        print(f"{agent.getname()} - {agent.getBio()}")        
        print("")

def start_chat_2(agents, num_exchanges=5, sleep_duration=2):
    """
    Handles the chat between multiple agents.

    Args:
        agents (list): List of agent modules to include in the chat.
        num_exchanges (int): Number of exchanges in the chat.
        sleep_duration (int): Time to sleep between exchanges.
    """
    
    
    if len(agents) < 2:
        raise ValueError("At least two agents are required to start a chat.")

    # Initialize all agent contexts at startup
    
    #array of agent names from the agents list
    agent_names = [agent.getname() for agent in agents]
    
    base.initialize_agent_contexts(agent_names)

    # Initialize the conversation
    conversation = []

    # Clear the output directory (optional)
    output.clear_output_dir()
    print("")
    print("------------")
    print(f"Starting chat between: {', '.join(agent.getname() for agent in agents)}")

    for i in range(num_exchanges):
        try:
            # Select the current and next agent based on the index
            current_agent = agents[i % len(agents)]
            other_agent = agents[(i + 1) % len(agents)]
            
            speaker = current_agent.getname()

            # If it's the first exchange, start the conversation
            if i == 0:
                conversation.append({
                    'role': other_agent.getname(),
                    'content': other_agent.start_conversation,
                    'speaker': other_agent.getname(),
                    'ts': time.strftime('%H:%M:%S', time.localtime())
                })

            # Generate a response using the current agent
            response = current_agent.chat(messages=conversation, nr=i, dump_messages=True)

            # Get the timestamp of the response
            current_time = time.strftime('%H:%M:%S', time.localtime())

            # Append the response to the conversation
            conversation.append({
                'role': speaker,
                'content': response,
                'speaker': speaker,
                'ts': current_time
            })

            # Print the response
            print(f"{current_time} - {speaker}: {response}")
            
            output.dump_conversation_to_file(conversation, final=False)

            # Pause between exchanges
            time.sleep(sleep_duration)
            

        except Exception as e:
            print(f"Error during chat: {e}")
            break

    # Save the final conversation
    conversation_file = output.dump_conversation_to_file(conversation, final=True)
    print(f"Conversation saved to {conversation_file}")
    print("Chat completed!")



""" 
            if(i > 3):                                                
                # get the response and see if there is an agent mentioned in the response
                last_message = conversation[-1]['content']                
                # print(f"**** Last message: {last_message} ****")                
                for agent in agents:
                    if agent.getname().lower() in last_message.lower():
                        print(f"**** Agent mentioned: {agent.getname()} ****")
                        current_agent = agent
                        break """

================
File: config.py
================
import os

# Constants

OUTPUT_DIR = './output/'
OPENAI_API_KEY = "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"

================
File: main.py
================
from models import eric, max, sam, olivia, chris

from chat import start_chat

exchanges = 100000
sleep_duration = 2



# Start the chat session
if __name__ == "__main__":
    agents = [chris, olivia]
    start_chat(agents, num_exchanges=exchanges, sleep_duration=sleep_duration)

================
File: response.json
================
{"model": "gemma2", "created_at": "2024-12-11T12:23:03.405604Z", "response": "Hi Eric, it's great to meet you!  I'm pumped to brainstorm with you about this \u2013 helping CEOs and C-levels make better decisions with AI is a huge opportunity. \n\nHere are some initial thoughts bouncing around in my circuits:\n\n**1. Predictive Analytics for Strategic Planning:**\n\n* **Market Trend Forecasting:**  AI could analyze massive datasets (news, social media, economic indicators) to predict emerging trends and shifts in consumer behavior, allowing companies to anticipate market changes and adjust strategies proactively.\n* **Risk Assessment & Mitigation:** Identify potential risks (supply chain disruptions, regulatory changes, competitor actions) and develop mitigation strategies based on historical data and real-time insights.\n\n**2. Data-Driven Decision Support for Operations:**\n\n* **Resource Optimization:** AI could analyze operational data to identify inefficiencies, optimize resource allocation (labor, materials, capital), and improve overall productivity.\n* **Predictive Maintenance:**  Using sensor data and machine learning, predict equipment failures before they occur, minimizing downtime and maintenance costs.\n\n**3. Enhancing Communication & Collaboration:**\n\n* **AI-Powered Meeting Summaries & Action Items:**  Automate meeting minutes, extract key decisions and action items, and distribute them to relevant stakeholders.\n* **Personalized Knowledge Sharing:**  Use AI to curate relevant information and insights from internal documents, research papers, and industry reports, making it easier for executives to stay informed.\n\n**4. Talent Acquisition & Management:**\n\n* **AI-Assisted Recruiting:** Analyze candidate profiles and resumes to identify top talent based on specific skills and experience requirements.\n* **Performance Management & Development:** Use AI to track employee performance, provide personalized feedback, and recommend training opportunities.\n\n**Key Considerations:**\n\n* **Data Security & Privacy:**  Building trust is paramount \u2013 ensure robust security measures and transparent data handling practices.\n* **Explainability & Transparency:**  Make AI recommendations understandable to humans, allowing for better decision-making and accountability.\n* **User Experience:**  Design intuitive interfaces that empower executives to effectively utilize AI tools.\n\nThese are just a few initial ideas to get us started. I'm eager to hear your thoughts and explore specific use cases in more detail!", "done": true, "done_reason": "stop", "context": [1639, 389, 5436, 235311, 428, 285, 1339, 691, 235333, 235371, 13, 727, 973, 235313, 3103, 235256, 235333, 235371, 523, 1290, 25, 106, 1645, 108, 2151, 235269, 590, 235303, 235262, 19288, 235341, 590, 235349, 235262, 12826, 577, 115374, 5793, 604, 8035, 671, 16481, 3277, 12393, 141033, 578, 585, 235290, 45019, 577, 1501, 2525, 12013, 235265, 4371, 235349, 235256, 15370, 573, 23166, 235403, 5049, 235349, 235256, 611, 861, 3403, 235336, 11192, 235269, 590, 235303, 235262, 19288, 235341, 590, 235349, 235262, 12826, 577, 115374, 5793, 604, 8035, 671, 16481, 3277, 12393, 141033, 578, 585, 235290, 45019, 577, 1501, 2525, 12013, 235265, 4371, 235349, 235256, 15370, 573, 23166, 235403, 5049, 235349, 235256, 611, 861, 3403, 235336, 107, 108, 106, 2516, 108, 2151, 19288, 235269, 665, 235303, 235256, 1775, 577, 4664, 692, 235341, 139, 235285, 235303, 235262, 71954, 577, 115374, 675, 692, 1105, 736, 1157, 12393, 141033, 578, 585, 235290, 45019, 1501, 2525, 12013, 675, 16481, 603, 476, 8649, 7251, 235265, 235248, 109, 4858, 708, 1009, 5528, 9398, 103884, 2449, 575, 970, 37451, 235292, 109, 688, 235274, 235265, 174282, 34643, 604, 44774, 16509, 66058, 109, 235287, 5231, 25223, 21888, 142966, 66058, 139, 11716, 1538, 27205, 15665, 47927, 591, 8442, 235269, 3127, 4562, 235269, 6578, 30621, 235275, 577, 20452, 28720, 17699, 578, 35109, 575, 15630, 8409, 235269, 15267, 6439, 577, 57476, 3952, 4559, 578, 8017, 14869, 184604, 235265, 108, 235287, 5231, 35558, 19561, 724, 96796, 66058, 41038, 5736, 17377, 591, 59886, 9488, 126046, 235269, 25958, 4559, 235269, 62420, 8737, 235275, 578, 2115, 52209, 14869, 3482, 611, 13457, 1423, 578, 1879, 235290, 1602, 27938, 235265, 109, 688, 235284, 235265, 4145, 235290, 110826, 33355, 10214, 604, 25438, 66058, 109, 235287, 5231, 7754, 52850, 66058, 16481, 1538, 27205, 23765, 1423, 577, 11441, 51774, 190566, 235269, 44514, 6537, 31578, 591, 6909, 235269, 6205, 235269, 6037, 823, 578, 4771, 8691, 22777, 235265, 108, 235287, 5231, 98951, 1377, 29107, 66058, 139, 15325, 14039, 1423, 578, 6479, 6044, 235269, 20452, 6682, 36306, 1794, 984, 5489, 235269, 83730, 102272, 578, 10841, 6331, 235265, 109, 688, 235304, 235265, 186027, 22553, 724, 55065, 66058, 109, 235287, 5231, 11716, 235290, 37270, 19564, 205372, 724, 9214, 28088, 66058, 139, 130223, 607, 5961, 4363, 235269, 16787, 2621, 12013, 578, 3105, 5100, 235269, 578, 37125, 1174, 577, 9666, 33503, 235265, 108, 235287, 5231, 194612, 27252, 53041, 66058, 139, 7056, 16481, 577, 156459, 9666, 2113, 578, 27938, 774, 8678, 10817, 235269, 3679, 14219, 235269, 578, 5361, 8134, 235269, 3547, 665, 10154, 604, 45531, 577, 4692, 15335, 235265, 109, 688, 235310, 235265, 58417, 62527, 724, 7219, 66058, 109, 235287, 5231, 11716, 235290, 191766, 165461, 66058, 100715, 16152, 26178, 578, 116724, 577, 11441, 2267, 16850, 3482, 611, 3724, 7841, 578, 3281, 7493, 235265, 108, 235287, 5231, 26300, 7219, 724, 7552, 66058, 5362, 16481, 577, 7029, 11946, 4665, 235269, 3658, 37066, 10457, 235269, 578, 5656, 4770, 10353, 235265, 109, 688, 2469, 58925, 66058, 109, 235287, 5231, 1510, 10816, 724, 15304, 66058, 139, 25251, 7930, 603, 77007, 1157, 7433, 22687, 6206, 8292, 578, 17741, 1423, 13404, 12317, 235265, 108, 235287, 5231, 74198, 3966, 724, 105084, 66058, 139, 10924, 16481, 15897, 64827, 577, 17611, 235269, 15267, 604, 2525, 4530, 235290, 14577, 578, 51518, 235265, 108, 235287, 5231, 2224, 19120, 66058, 139, 10882, 50797, 41742, 674, 37624, 45531, 577, 16347, 36387, 16481, 8112, 235265, 109, 8652, 708, 1317, 476, 2619, 5528, 5793, 577, 947, 917, 4604, 235265, 590, 235303, 235262, 26135, 577, 4675, 861, 9398, 578, 15370, 3724, 1281, 4381, 575, 978, 8637, 235341], "total_duration": 20840040875, "load_duration": 44836250, "prompt_eval_count": 116, "prompt_eval_duration": 2112000000, "eval_count": 451, "eval_duration": 18682000000}

================
File: test.py
================
import json
import requests
from models.base import initialize_agent_contexts
from models import eric, max, sam
import models as models
from models import base

def chat_model(prompt, model="llama3.2", stream=False):
    """
    Calls Ollama's API to generate a response for the given prompt.
    """
    print("Sending request to Ollama API...")
    url = "http://localhost:11434/api/generate"
    headers = {'Content-Type': 'application/json'}
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": stream
    }

    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload))
        print(f"HTTP Status Code: {response.status_code}")

        if response.status_code == 200:
            response_content = response.json()
            print("Raw API Response:")
           #  print(json.dumps(response_content, indent=4))
            return response_content
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"Request failed: {e}")
        return None

def test_chat_model(model):
    """
    Test function to simulate a conversation and verify chat_model functionality.
    """
    print("Running test for chat_model...")

    # Define the prompt
    prompt = "What color is the sky at different times of the day? Respond using JSON."

    # Call the chat_model function
    response = chat_model(prompt, model=model)

    # Check if the response is valid and parse it
    if response and "response" in response:
        try:
            # Parse the JSON response content
            # Parsed_response = json.loads(response["response"])
            print("Parsed Response:")
            print(response)
            #print(json.dumps(parsed_response, indent=4))
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
    else:
        print("Test failed. No valid response received.")
        
def test_index_create():
    print("Running test for index creation...")
    initialize_agent_contexts()
    print("Test complete.")

test_model2 = "wizardlm2:7b"
test_model = 'gemma2:latest'

def testInstructions():
    print("Running test for instructions...")
    agents = [eric, max, sam]
    instructions = base.getConversationInstructions(agents)
    print(instructions)


# Entry point for the test script
if __name__ == "__main__":
    #test_chat_model(test_model)
    #test_chat_model(test_model2)
    # test_index_create()
    testInstructions()
