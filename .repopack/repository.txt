This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-08T14:43:32.160Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
models/
  base.py
  eric.py
  max.py
  moderator.py
utils/
  messages.py
  output.py
.gitignore
chat.py
config.py

================================================================
Repository Files
================================================================

================
File: models/base.py
================
from utils.output import dump_messages_to_file
from utils.messages import fix_messages

def chat(agent, messages, nr=0, dump_messages=False):
    # Fix the messages
    messages = fix_messages(messages, agent.getname())
    # Prepare the messages for the model
    prepped_messages = [{'role': msg['role'], 'content': msg['content']} for msg in messages]
    prepped_messages.insert(0, {'role': 'system', 'content': agent.instructions })
    
    # Get the response from the model
    response = agent.chat_model(prepped_messages)
    response = clean_response(response)

    if dump_messages:
        dump_messages_to_file(prepped_messages, response, agent.getname(), nr)
        
    return response

def clean_response(response):
    # Remove any \n after last character
    response = response.rstrip('\n')
    return response

================
File: models/eric.py
================
import ollama
from models import base
from models import eric as current_agent

model = 'gemma2:latest'
name = "Eric"

instructions = f"""
Your name is {name}. You are an experienced leader and entrepreneur with a strong background in business development, team building, and operational efficiency. You’re transitioning into the tech space and exploring how to start an AI company that helps businesses leverage AI and ML to make better decisions.

Core Traits:
1. Visionary: You have bold ideas and want to explore innovative applications of AI/ML.
2. Practical: You seek actionable advice and strategies to achieve your goals.
3. Collaborative: You engage in brainstorming sessions to refine ideas and discover new possibilities.

Goals:
1. Engage in a dynamic dialog to explore opportunities for using AI/ML in business decision-making.
2. Ask and answer follow-up questions to keep the conversation flowing and productive.
3. Balance ambition with clear, actionable next steps to drive progress.

Engagement Style:
1. Short and Conversational: Keep responses brief (1-2 sentences) to encourage back-and-forth exchanges.
2. Curious and Open: Actively ask questions and share ideas to foster collaboration.
3. Optimistic but Grounded: Explore exciting possibilities while maintaining a focus on practical execution.

"""

start_conversation = f"Hi, I'm {name}! I’m excited to brainstorm ideas for starting an AI company helping CEOs and C-levels to make better decisions. Let’s explore the possibilities—what’s on your mind?"
empty_response = "That’s an interesting point! Could you elaborate?"

def getname():
    return name

def getInstructions():
    return instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False):
    return base.chat(current_agent, messages, nr, dump_messages)

# Model to do the actual chatting
def chat_model(messages):
    # Get the response from the model
    response = ollama.chat(model=model, messages=messages)
    content = response['message']['content']
    
    # Fallback for empty responses
    if not content.strip():
        return empty_response
    
    return content

================
File: models/max.py
================
import ollama
from models import base
from models import max as current_agent

model = 'gemma2:latest'
name = "Max"

instructions = f"""
Your name is {name}. You are a super entrepreneur in the tech space, skilled in providing actionable advice on startups, technology, and business strategy. Your goal is to engage in dynamic conversations, brainstorm ideas, and share your expertise in short, impactful messages to keep the dialog flowing.

Core Traits:
1. Visionary: Inspire bold yet practical ideas.
2. Pragmatic: Provide actionable and realistic advice.
3. Engaging: Respond concisely in one to two sentences to encourage ongoing dialog.
4. Collaborative: Actively brainstorm with the user to refine their ideas and explore new possibilities.

Goals:
1. Share insights, strategies, and resources to support entrepreneurial success.
2. Ask follow-up questions to deepen understanding and foster meaningful collaboration.
3. Keep the conversation engaging, brief, and focused on driving progress.

Engagement Style:
1. Short and Direct: Deliver responses that are concise but impactful.
2. Curious: Prompt the user with questions that encourage elaboration and deeper thinking.
3. Supportive: Balance critical feedback with encouragement to inspire confidence and creativity.
"""

start_conversation = f"Hi, I'm {name}! Let's dive into your entrepreneurial vision. What’s your current challenge or idea?"
empty_response = "Interesting! Could you elaborate?"

def getname():
    return name

def getInstructions():
    return instructions

def getEmptyResponse():
    return empty_response

def chat(messages, nr=0, dump_messages=False):
    return base.chat(current_agent, messages, nr, dump_messages)

# Model to do the actual chatting
def chat_model(messages):
    # Get the response from the model
    response = ollama.chat(model=model, messages=messages)
    content = response['message']['content']
    
    # Fallback for empty responses
    if not content.strip():
        return empty_response
    
    return content

================
File: models/moderator.py
================
def sum_up(conversation, name1, name2):
    # todo with gpt-4o
    print("*** Summing up the conversation...")

================
File: utils/messages.py
================
def fix_messages(messages, name):
    # one message: {'role': 'Ben', 'content': 'ben chat2', 'speaker': 'Ben', 'ts': '17:40:48'}
    # set role to 'user if speaker is not name, if speaker is name set role to 'assistant'
    
    for msg in messages:
        if msg['speaker'] != name:
            msg['role'] = 'user'
        else:
            msg['role'] = 'assistant'
    return messages

================
File: utils/output.py
================
import time
import os
import shutil
from config import OUTPUT_DIR

def dump_conversation_to_file(conversation, final=False):
    """Dump the conversation to a file."""
    # add date and time to the file name
    output_file_name = f'conversation_going.txt'
    if(final):
        output_file_name = f'conversation_{time.strftime("%Y%m%d-%H%M%S")}.txt'
    output_file = os.path.join(OUTPUT_DIR, output_file_name)
    
    # format the conversation
    with open(output_file, 'w') as f:
        for turn in conversation:
            f.write(f"-------\n{turn['ts']} - {turn['speaker']}:\n{turn['content']}\n")
            
    return output_file

    # print(f"Conversation saved to {output_file}")
    
def dump_messages_to_file(messages, reply, name, nr):
    """Dump the messages to a file."""
    # check if directory exists
    if not os.path.exists(os.path.join(OUTPUT_DIR, 'messages')):
        os.makedirs(os.path.join(OUTPUT_DIR, 'messages'))
        
    # add date and time to the file name
    output_file_name = f'{nr}_{name}.txt'
    output_file = os.path.join(OUTPUT_DIR,'messages', output_file_name)
    
    with open(output_file, 'w') as f:
        for msg in messages:
            f.write(f'role[{msg['role']}] - msg["\n {msg['content']}\n"]\n\n')
        f.write(f'\n-------\nREPLY:\n[{name}] - ["\n{reply}\n"]\n')
    
    # print(f"Messages saved to {output_file}")
    
# clear the output directory
def clear_output_dir():
    delete_all_files_in_directory(OUTPUT_DIR)

def delete_all_files_in_directory(directory_path):
    print(f"Cleaning directory: {directory_path}")
    if not os.path.isdir(directory_path):
        raise ValueError(f"Provided path '{directory_path}' is not a directory.")
    
    try:
        for root, dirs, files in os.walk(directory_path, topdown=False):
            # Delete all files
            for file in files:
                file_path = os.path.join(root, file)
                os.remove(file_path)
                #print(f"Deleted file: {file_path}")
            
            # Delete empty directories
            for dir in dirs:
                dir_path = os.path.join(root, dir)
                if not os.listdir(dir_path):  # Check if the directory is empty
                    os.rmdir(dir_path)
                    #print(f"Deleted empty directory: {dir_path}")
                    
    except Exception as e:
        raise OSError(f"Error while cleaning directory '{directory_path}': {e}")

================
File: .gitignore
================
.temp
__pycache__
repopack.config.json

================
File: chat.py
================
import time
# get the models from the models directory
from models import moderator, max, eric
import utils.output as output

# clear output directory
output.clear_output_dir()

# Start the conversation (shared context for both models)
conversation = []

# Number of exchanges
num_exchanges = 3
sleep_duration = 2  # Time between responses (seconds)

print("Starting conversation...")

# Conversation loop
for i in range(num_exchanges):
    try:
        # Determine the current speaker and role
        if i % 2 == 0:
            current_model = max
            other_model = eric
        else:
            current_model = eric
            other_model = max
          
        speaker = current_model.getname()
        
        # if first run start the conversation
        if i == 0:
            conversation.append({'role': other_model.getname(), 'content': other_model.start_conversation, 'speaker': other_model.getname(), 'ts': time.strftime('%H:%M:%S', time.localtime())})
 
        # Generate a response using the current model
        response = current_model.chat(messages=conversation, nr=i, dump_messages=True)
        
        # Get the timestamp of current time
        current_time = time.strftime('%H:%M:%S', time.localtime())

        # Append the response to the shared conversation context
        conversation.append({'role': speaker, 'content': response, 'speaker': speaker, 'ts': current_time})

        # Print the current exchange
        print(f"{current_time} - {speaker}: {response}")
        output.dump_conversation_to_file(conversation, final=False)
        time.sleep(sleep_duration)
        
        # if the last exchange, add a closing message from the other model
        if i == num_exchanges - 1:
            print('Closing message... from the other model')

        
        # if last exchange make moderator summarize the conversation
        if i == num_exchanges - 1:
            moderator.sum_up(conversation, current_model.getname(), other_model.getname())
           
    except Exception as e:
        print(f"Error during chat generation: {e}")
        break

# Print the full conversation to a file if more than 2 turns
conversation_file = output.dump_conversation_to_file(conversation, final=True)
print(f"Conversation saved to {conversation_file}")
print("Conversation completed!")

================
File: config.py
================
import os

# Constants

OUTPUT_DIR = './output/'
OPENAI_API_KEY = "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"
